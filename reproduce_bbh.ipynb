{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lorahub.algorithm import lorahub_inference\n",
    "import os\n",
    "import json\n",
    "from lorahub.algorithm2 import lorahub_learning, lorahub_inference\n",
    "import lorahub.algorithm2 as algorithm2\n",
    "import lorahub.algorithm3_grad as algorithm3_grad\n",
    "from lorahub.constant import LORA_MODULE_NAMES\n",
    "import random\n",
    "from random import shuffle\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_flan_results_zero_shot(folder, flan_model_name):\n",
    "    sub_dirs = os.listdir(folder)\n",
    "    result={}\n",
    "    for sub_dir in sub_dirs:\n",
    "        test_file_path = os.path.join(folder, sub_dir, \"zero_shot.jsonl\")\n",
    "        task_inputs, task_outputs = [], []\n",
    "        for line in open(test_file_path, \"r\", encoding=\"utf-8\"):\n",
    "            example = json.loads(line)\n",
    "            task_inputs.append(example[\"context\"])\n",
    "            task_outputs.append(example[\"completion\"])\n",
    "        print(\"Evaluating on task (zero shot): \", sub_dir)\n",
    "        _,task_acc=lorahub_inference(task_inputs,\n",
    "                          flan_model_name,\n",
    "                          flan_model_name,\n",
    "                          16,\n",
    "                          task_outputs)\n",
    "        print(\"task accuracy:\",task_acc)\n",
    "        # lorahub_inference(task_inputs,\n",
    "        #                   flan_model_name,\n",
    "        #                   flan_model_name,\n",
    "        #                   16,\n",
    "        #                   task_outputs)\n",
    "        result[sub_dir]=task_acc\n",
    "    result_pd=pd.DataFrame({'Zero-shot acc':result})\n",
    "    return result,result_pd\n",
    "        # break\n",
    "\n",
    "\n",
    "def evaluate_flan_results_few_shot(folder, flan_model_name):\n",
    "    sub_dirs = os.listdir(folder)\n",
    "    result={}\n",
    "    for sub_dir in sub_dirs:\n",
    "        test_file_path = os.path.join(folder, sub_dir, \"few_shot.jsonl\")\n",
    "        task_inputs, task_outputs = [], []\n",
    "        for line in open(test_file_path, \"r\", encoding=\"utf-8\"):\n",
    "            example = json.loads(line)\n",
    "            task_inputs.append(example[\"context\"])\n",
    "            task_outputs.append(example[\"completion\"])\n",
    "        print(\"Evaluating on task (five shot): \", sub_dir)\n",
    "        _,task_acc=lorahub_inference(task_inputs,\n",
    "                          flan_model_name,\n",
    "                          flan_model_name,\n",
    "                          16,\n",
    "                          task_outputs)\n",
    "        result[sub_dir]=task_acc\n",
    "    result_pd=pd.DataFrame({'Few-shot acc':result})\n",
    "    return result,result_pd\n",
    "\n",
    "7\n",
    "def evaluate_lorahub_results_few_shot(folder, flan_model_name):\n",
    "    sub_dirs = os.listdir(folder)\n",
    "    result={'lorahub avg acc':{},'lorahub max acc':{}}\n",
    "    # 5 seeds used in our experiments\n",
    "    for sub_dir in sub_dirs:\n",
    "        print(\"Evaluating on task (lorahub): \", sub_dir)\n",
    "        # construct the few-shot examples for lorahub learning\n",
    "        example_inputs, examples_outputs = [], []\n",
    "        example_file_path = os.path.join(folder, sub_dir, \"example.jsonl\")\n",
    "        for line in open(example_file_path, \"r\", encoding=\"utf-8\"):\n",
    "            example = json.loads(line)\n",
    "            example_inputs.append(example[\"context\"])\n",
    "            examples_outputs.append(example[\"completion\"])\n",
    "            \n",
    "        # random select 5 examples for each task\n",
    "        random.seed(42)\n",
    "        shuffled_set = list(zip(example_inputs, examples_outputs))\n",
    "        random.shuffle(shuffled_set)\n",
    "        example_inputs, examples_outputs = zip(*shuffled_set)\n",
    "        # take the first 5 examples\n",
    "        example_inputs, examples_outputs = example_inputs[:5], examples_outputs[:5]\n",
    "\n",
    "        # load the zero-shot examples for evaluation\n",
    "        test_file_path = os.path.join(folder, sub_dir, \"zero_shot.jsonl\")\n",
    "        task_inputs, task_outputs = [], []\n",
    "        for line in open(test_file_path, \"r\", encoding=\"utf-8\"):\n",
    "            example = json.loads(line)\n",
    "            task_inputs.append(example[\"context\"])\n",
    "            task_outputs.append(example[\"completion\"])\n",
    "\n",
    "        task_perf_list = []\n",
    "        for seed in range(1, 6):\n",
    "            random.seed(seed)\n",
    "            print(\"Evaluating on task (lorahub): \", sub_dir, \"with seed:\", seed)\n",
    "\n",
    "            def get_lora_module_list():\n",
    "                return random.sample(LORA_MODULE_NAMES, 20) #what \n",
    "            # get a list of modules to be used in the composition\n",
    "            modules = get_lora_module_list()\n",
    "\n",
    "            # perform LoRAHub learning\n",
    "            module_weights, model, tokenizer = lorahub_learning(lora_module_list=modules,\n",
    "                                                                example_inputs=example_inputs,\n",
    "                                                                example_outputs=examples_outputs,\n",
    "                                                                max_inference_step=40,\n",
    "                                                                batch_size=5)\n",
    "\n",
    "            print(\"module_weights:\", module_weights)\n",
    "\n",
    "            \"\"\"\n",
    "            Perform inference to get predictions\n",
    "            \"\"\"\n",
    "            _, task_acc = lorahub_inference(example_inputs=task_inputs,\n",
    "                                            model_or_name_path=model,\n",
    "                                            tokenizer_or_tokenizer_path=tokenizer,\n",
    "                                            batch_size=10,\n",
    "                                            # can set as None if you do not have the ground truth\n",
    "                                            example_outputs=task_outputs)\n",
    "            task_perf_list.append(task_acc)\n",
    "            break\n",
    "        avg_perf, max_perf = sum(task_perf_list) / len(task_perf_list), max(task_perf_list)\n",
    "        print(\"average perf:\", avg_perf, \"best perf:\", max_perf)\n",
    "        result[\"lorahub avg acc\"][sub_dir]=avg_perf\n",
    "        result[\"lorahub max acc\"][sub_dir]=max_perf\n",
    "        break\n",
    "    result_pd=pd.DataFrame(result)\n",
    "    return result,result_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"data_bbh\"):\n",
    "#     # download dataset\n",
    "#     os.system(\"wget https://github.com/sail-sg/lorahub/releases/download/0.1/data_bbh.zip\")\n",
    "#     # unzip\n",
    "#     os.system(\"unzip data_bbh.zip\")\n",
    "# evaluate the model\n",
    "result_folder = \"results\"\n",
    "zero_result,zero_result_df=evaluate_flan_results_zero_shot(\"data_bbh\", \"google/flan-t5-large\")\n",
    "zero_result_df.to_csv(os.path.join(result_folder, \"zero_result.csv\"))\n",
    "\n",
    "# five shot for flan models\n",
    "few_result,few_result_df=evaluate_flan_results_few_shot(\"data_bbh\", \"google/flan-t5-large\")\n",
    "few_result_df.to_csv(os.path.join(result_folder, \"few_result.csv\"))\n",
    "# five shot for lorahub models\n",
    "lorahub_result,lorahub_result_df=evaluate_lorahub_results_few_shot(\"data_bbh\", \"google/flan-t5-large\")\n",
    "lorahub_result_df.to_csv(os.path.join(result_folder, \"lorahub_result.csv\"))\n",
    "\n",
    "#save all the results to a csv file\n",
    "result_df=pd.DataFrame({'zero-shot acc':zero_result,'few-shot acc':few_result,'lorahub avg acc':lorahub_result['lorahub avg acc'],'lorahub max acc':lorahub_result['lorahub max acc']})\n",
    "result_df.to_csv(os.path.join(result_folder, \"result.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorahub_df = pd.read_csv('results/lorahub_result.csv',index_col=0).sort_index()\n",
    "few_df = pd.read_csv('results/few_result.csv',index_col=0).sort_index()\n",
    "zero_df = pd.read_csv('results/zero_result.csv',index_col=0).sort_index()\n",
    "print(lorahub_df)\n",
    "# Merge the data frames\n",
    "merged_df = pd.concat([lorahub_df, few_df, zero_df], axis=1)\n",
    "\n",
    "# # Display the merged dataframe\n",
    "print(merged_df)\n",
    "\n",
    "# Optionally, save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('results/merged_result.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on task (lorahub):  boolean_expressions\n"
     ]
    }
   ],
   "source": [
    "folder = \"data_bbh\"\n",
    "flan_model_name = \"google/flan-t5-large\"\n",
    "sub_dirs = os.listdir(folder)\n",
    "result={'lorahub avg acc':{},'lorahub max acc':{}}\n",
    "sub_dir=sub_dirs[0]\n",
    "# 5 seeds used in our experiments\n",
    "print(\"Evaluating on task (lorahub): \", sub_dir)\n",
    "# construct the few-shot examples for lorahub learning\n",
    "example_inputs, examples_outputs = [], []\n",
    "example_file_path = os.path.join(folder, sub_dir, \"example.jsonl\")\n",
    "for line in open(example_file_path, \"r\", encoding=\"utf-8\"):\n",
    "    example = json.loads(line)\n",
    "    example_inputs.append(example[\"context\"])\n",
    "    examples_outputs.append(example[\"completion\"])\n",
    "    \n",
    "# random select 5 examples for each task\n",
    "random.seed(42)\n",
    "shuffled_set = list(zip(example_inputs, examples_outputs))\n",
    "random.shuffle(shuffled_set)\n",
    "example_inputs, examples_outputs = zip(*shuffled_set)\n",
    "# take the first 5 examples\n",
    "example_num=100\n",
    "example_inputs, examples_outputs = example_inputs[:example_num], examples_outputs[:example_num]\n",
    "\n",
    "# load the zero-shot examples for evaluation\n",
    "test_file_path = os.path.join(folder, sub_dir, \"zero_shot.jsonl\")\n",
    "task_inputs, task_outputs = [], []\n",
    "for line in open(test_file_path, \"r\", encoding=\"utf-8\"):\n",
    "    example = json.loads(line)\n",
    "    task_inputs.append(example[\"context\"])\n",
    "    task_outputs.append(example[\"completion\"])\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "def get_lora_module_list():\n",
    "        return random.sample(LORA_MODULE_NAMES, 100) #what \n",
    "    # get a list of modules to be used in the composition\n",
    "\n",
    "modules = get_lora_module_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reimport lorahub.algorithm2\n",
    "import importlib\n",
    "import lorahub.algorithm3_grad_old as algorithm3_grad_old\n",
    "importlib.reload(algorithm3_grad)\n",
    "importlib.reload(algorithm3_grad_old)\n",
    "from lorahub.algorithm3_grad import lorahub_learning, lorahub_inference\n",
    "# from lorahub.algorithm3_grad_old import lorahub_learning, lorahub_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on task (lorahub):  boolean_expressions with seed: 1\n",
      "> Begin to load lora modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-glue_mrpc ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:55,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-dream_baseline ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:01<00:54,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-ropes_given_background_situation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:01<00:53,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quarel_logic_test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:02<00:53,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-anli_r1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:02<00:52,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-imdb_reviews_plain_text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:03<00:53,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-race_high_Is_this_the_right_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:03<00:53,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-ropes_plain_no_background ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:04<00:53,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quail_context_question_description_answer_text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:05<01:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-kilt_tasks_hotpotqa_final_exam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:06<00:58,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quartz_read_passage_below_choose ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:06<00:57,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_hop_original_generate_subject_and_object ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:07<00:57,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_hop_original_choose_best_object_interrogative_2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:07<00:56,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_qa_exercise ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:08<00:57,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_hop_original_choose_best_object_affirmative_3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:09<01:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-duorc_SelfRC_title_generation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:10<00:59,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-true_case ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:10<00:55,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-adversarial_qa_dbert_question_context_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:11<00:53,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-super_glue_wsc.fixed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:12<00:52,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-glue_mnli ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:12<00:50,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-ropes_prompt_bottom_hint_beginning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:13<00:51,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-race_middle_Write_a_multi_choice_question_options_given_ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:13<00:49,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-sciq_Multiple_Choice ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:14<00:49,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quartz_paragraph_question_plain_concat ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:15<00:48,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiqa_what_is_the_missing_first_step ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:16<00:53,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-squad_v2.0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:17<01:05,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-amazon_polarity_would_you_buy ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:18<01:12,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-duorc_ParaphraseRC_question_answering ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:19<01:05,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-ropes_plain_bottom_hint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:19<00:58,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiqa_effect_with_string_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:20<00:53,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-ropes_read_background_situation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:21<00:49,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-ag_news_subset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:21<00:46,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quoref_Found_Context_Online ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:22<00:45,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quail_no_prompt_id ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [00:23<00:44,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quoref_Find_Answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:23<00:41,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quoref_Answer_Test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:24<00:40,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wmt16_translate_tr-en ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:24<00:39,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-duorc_SelfRC_extract_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [00:25<00:40,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-qasc_is_correct_2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:26<00:38,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_bio_what_content ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:26<00:37,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-drop ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:27<00:36,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quail_context_question_description_answer_id ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:27<00:34,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_qa_Topic_Prediction_Answer_Only ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:28<00:34,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-qasc_qa_with_separated_facts_5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:29<00:34,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-app_reviews_convert_to_rating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:29<00:33,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quail_context_description_question_answer_id ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:30<00:36,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-adversarial_qa_droberta_question_context_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:31<00:38,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quail_context_question_answer_description_text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:32<00:35,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-duorc_SelfRC_generate_question_by_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:32<00:37,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-adversarial_qa_dbert_based_on ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:33<00:37,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-race_high_Select_the_best_answer_no_instructions_ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:34<00:34,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-social_i_qa_Show_choices_and_generate_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:35<00:34,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiqa_effect_with_label_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:35<00:32,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-sciq_Multiple_Choice_Closed_Book_ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [00:36<00:31,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quoref_Guess_Title_For_Context ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:36<00:29,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_bio_who ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [00:37<00:28,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_hop_original_choose_best_object_interrogative_1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:38<00:27,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-duorc_ParaphraseRC_decide_worth_it ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:38<00:27,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-sciq_Direct_Question ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:39<00:28,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-social_i_qa_Generate_the_question_from_the_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:40<00:27,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quartz_use_info_from_paragraph_question ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:40<00:26,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-social_i_qa_Show_choices_and_generate_index ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:41<00:25,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quartz_given_the_fact_answer_the_q ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:42<00:24,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-duorc_SelfRC_decide_worth_it ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [00:42<00:23,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-duorc_ParaphraseRC_movie_director ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:43<00:22,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-anli_r2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [00:44<00:22,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_hop_original_explain_relation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:44<00:21,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_qa_Decide_good_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [00:45<00:20,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-ropes_prompt_beginning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [00:46<00:19,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-fix_punct ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:46<00:18,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-ropes_prompt_mix ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:47<00:17,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-web_questions_potential_correct_answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [00:47<00:17,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiqa_what_is_the_final_step_of_the_following_process ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:48<00:17,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quail_description_context_question_answer_text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [00:49<00:16,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-squad_v1.1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [00:49<00:15,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-amazon_polarity_user_satisfied ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [00:50<00:14,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-amazon_polarity_convey_negative_or_positive_sentiment ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [00:51<00:15,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_hop_original_generate_subject ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [00:51<00:14,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-yelp_polarity_reviews ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [00:52<00:13,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quoref_Guess_Answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [00:53<00:13,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-app_reviews_categorize_rating_using_review ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [00:53<00:12,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-super_glue_cb ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:54<00:12,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-paws_wiki ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:55<00:11,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quac ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [00:55<00:10,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-duorc_SelfRC_question_answering ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [00:56<00:09,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-adversarial_qa_droberta_based_on ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:57<00:08,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-race_high_Taking_a_test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [00:57<00:08,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quartz_use_info_from_question_paragraph ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [00:58<00:07,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_bio_guess_person ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [00:58<00:07,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_qa_automatic_system ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [00:59<00:06,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-race_high_Select_the_best_answer_generate_span_ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [01:00<00:05,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-adversarial_qa_dbidaf_tell_what_it_is ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [01:00<00:05,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_qa_Jeopardy_style ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [01:01<00:04,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-anli_r3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [01:02<00:04,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-qasc_qa_with_combined_facts_1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [01:02<00:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-glue_cola ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [01:03<00:02,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-adversarial_qa_droberta_answer_the_following_q ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [01:04<00:01,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-wiki_qa_Is_This_True_ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [01:04<00:01,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quoref_What_Is_The_Answer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [01:05<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading lorahub/flan_t5_large-quail_context_description_question_answer_text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb21b5fed044af8adf94b015acf3900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_blocks: 48\n",
      "> Begin to perform gradient optimization ...\n",
      "Step 0, loss 0.05648682203463977\n",
      "Step 1, loss 0.037820473246392795\n",
      "Step 2, loss 0.030990637662034713\n",
      "Step 3, loss 0.022491906974755692\n",
      "Step 4, loss 0.07842005029233405\n",
      "Step 5, loss 0.013905840214283672\n",
      "Step 6, loss 0.030855559828341938\n",
      "Step 7, loss 0.015784347451699432\n",
      "Step 8, loss 0.0074658355326391755\n",
      "Step 9, loss 0.004477370080712717\n",
      "Step 10, loss 0.0028234606550313402\n",
      "Step 11, loss 0.010425313632867982\n",
      "Step 12, loss 0.014777394551492762\n",
      "Step 13, loss 0.007724030695226247\n",
      "Step 14, loss 0.00542082952824785\n",
      "Step 15, loss 0.0010344053202743452\n",
      "Step 16, loss 0.0007996597053647747\n",
      "Step 17, loss 0.00038478464636568786\n",
      "Step 18, loss 0.00017636726213936526\n",
      "Step 19, loss 0.00015627636638981812\n",
      "Step 20, loss 0.0001497667541130454\n",
      "Step 21, loss 0.0001458272687912654\n",
      "Step 22, loss 0.00014287373603991683\n",
      "Step 23, loss 0.0001405190570267223\n",
      "Step 24, loss 0.00013860059200254682\n",
      "Step 25, loss 0.0001369847688977188\n",
      "Step 26, loss 0.0001356042931647039\n",
      "Step 27, loss 0.00013441037130377254\n",
      "Step 28, loss 0.00013336808298873847\n",
      "Step 29, loss 0.00013244715138114316\n",
      "module_weights: [[ 0.09008881 -0.1653535   0.3263653  ...  0.16518393 -0.07606521\n",
      "  -0.24345611]\n",
      " [ 0.64855057  0.00354531  0.15318994 ...  0.3177372  -0.08007601\n",
      "  -0.0489042 ]\n",
      " [ 0.400965   -0.37424502 -0.25097117 ...  0.20044832  0.04149611\n",
      "   0.05480478]\n",
      " ...\n",
      " [-0.6874192   0.6487822   0.08523294 ...  0.17917839 -0.12127399\n",
      "  -0.08878346]\n",
      " [ 0.6104384   0.1926179   0.4977136  ...  0.06032096  0.11786529\n",
      "   0.04504678]\n",
      " [ 0.5157367   0.4255638  -0.11854864 ...  0.3579886  -0.18167338\n",
      "   0.08121338]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dad6c4fe4d9410c86bdb578147df810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GJK\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2778: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average perf: 84.0 best perf: 84.0\n"
     ]
    }
   ],
   "source": [
    "task_perf_list = []\n",
    "seed=1\n",
    "print(\"Evaluating on task (lorahub): \", sub_dir, \"with seed:\", seed)\n",
    "\n",
    "\n",
    "\n",
    "# perform LoRAHub learning\n",
    "module_weights, model, tokenizer = lorahub_learning(lora_module_list=modules,\n",
    "                                                    example_inputs=example_inputs,\n",
    "                                                    example_outputs=examples_outputs,\n",
    "                                                    max_inference_step=30,\n",
    "                                                    batch_size=5,\n",
    "                                                    early_stopping=True,\n",
    "                                                    lr=0.02)\n",
    "\n",
    "print(\"module_weights:\", module_weights)\n",
    "\n",
    "\"\"\"\n",
    "Perform inference to get predictions\n",
    "\"\"\"\n",
    "_, task_acc = lorahub_inference(example_inputs=task_inputs,\n",
    "                                model_or_name_path=model,\n",
    "                                tokenizer_or_tokenizer_path=tokenizer,\n",
    "                                batch_size=10,\n",
    "                                 # can set as None if you do not have the ground truth\n",
    "                                example_outputs=task_outputs)\n",
    "task_perf_list.append(task_acc)\n",
    "avg_perf, max_perf = sum(task_perf_list) / len(task_perf_list), max(task_perf_list)\n",
    "print(\"average perf:\", avg_perf, \"best perf:\", max_perf)\n",
    "result[\"lorahub avg acc\"][sub_dir]=avg_perf\n",
    "result[\"lorahub max acc\"][sub_dir]=max_perf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
